{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from numpy import array, argmax, random, take\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import RNN, SimpleRNN, LSTM,  Embedding, RepeatVector\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "#For plotting the matplotlib graphs in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_path=\"D:\\\\Google Drive\\\\Training\\\\Book\\\\0.Chapters\\\\Chapter12 RNN and LSTM\\\\Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "column_names = ['word1', 'word2', 'word3']\n",
    "\n",
    "input_3gram = pd.read_csv(Data_path+ \"\\\\3Gram_love_data.txt\", delimiter='\\t', names=column_names) #Importing csv file with column names\n",
    "print(\"shape of data\", input_3gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Few sample records from data \\n\", input_3gram.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFrequency of word1 values \\n\", input_3gram[\"word1\"].value_counts())\n",
    "print(\"\\nFrequency of word2 values \\n\", input_3gram[\"word2\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding our words to create dictionary\n",
    "Here we find unique values in each column and save each of those values .\n",
    "Later which we will take the unique value for the entire appened columns\n",
    "This will be our vocabulary list,which are the unique words in our data file\n",
    "\"\"\"\n",
    "unique_words = []\n",
    "for i in list(input_3gram.columns.values):\n",
    "    for j in pd.unique(input_3gram[i]):\n",
    "        unique_words.append(j)\n",
    "unique_words = np.unique(unique_words)\n",
    "\n",
    "\n",
    "print('Count of unique words overall:', len(unique_words))\n",
    "print('unique words list:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creating our word:indice pair dictionary and inverse\n",
    "Here will be creating two dictonary values\n",
    "word_indices : This contains each words mapped to an unique digit \n",
    "indices_words : This contains each digits mapped to a word in the same sequence as word_indices \n",
    "\"\"\"\n",
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_words = dict((i, w) for i, w in enumerate(unique_words))\n",
    "\n",
    "print(\"word_indices dictionary \\n\",word_indices)\n",
    "print(\"word_indices.keys \\n\", word_indices.keys())\n",
    "print(\"word_indices.values \\n\", word_indices.values())\n",
    "print(\"\\n ########################################\\n\")\n",
    "print(\"indices_words dictionary \\n\", indices_words)\n",
    "print(\"indices_words keys \\n\",indices_words.keys())\n",
    "print(\"indices_words values \\n\",indices_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Onehot encoding of word1\n",
    "word1 = input_3gram['word1'].map(word_indices)\n",
    "word1_onehot = keras.utils.to_categorical(np.array(word1), num_classes=len(word_indices))\n",
    "print(\"word1_onehot shape is \",word1_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take example of two different words\n",
    "print(\"The word in row 0 is -->\"+input_3gram['word1'][0])\n",
    "print(\"The one hot encoded version of the word in row 0 is \\n\",word1_onehot[0])\n",
    "\n",
    "print(\"\\nThe word in row 500 is --> \"+input_3gram['word1'][500])\n",
    "print(\"The one hot encoded version of the word in row 500 is \\n\",word1_onehot[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##one hot encoding for word2 and word3 \n",
    "word2 = input_3gram['word2'].map(word_indices)\n",
    "word2_onehot = keras.utils.to_categorical(np.array(word2), num_classes=len(word_indices))\n",
    "print(\"word2_onehot shape is \",word2_onehot.shape)\n",
    "\n",
    "word3 = input_3gram['word3'].map(word_indices)\n",
    "word3_onehot = keras.utils.to_categorical(np.array(word3), num_classes=len(word_indices))\n",
    "print(\"word3_onehot shape is \",word3_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model1 = Sequential()\n",
    "ANN_model1.add(Dense(10, input_dim=word1_onehot.shape[1], activation='sigmoid'))\n",
    "ANN_model1.add(Dense(word2_onehot.shape[1] ,activation='softmax'))\n",
    "ANN_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model1.fit(word1_onehot, word2_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will see what the 1st hidden layer output representation of the data  \n",
    "# to predict the hidden layer activations, \n",
    "# let's rewrite first layer of our model and give it the weights from fully trained previous model\n",
    "model1_hidden = Sequential()\n",
    "model1_hidden.add(Dense(10, input_dim=word1_onehot.shape[1], weights=ANN_model1.layers[0].get_weights()))\n",
    "model1_hidden.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the hidden layer activations\n",
    "model1_hidden_output = model1_hidden.predict(word1_onehot)\n",
    "#peak into our hidden layer activations\n",
    "print(\"The hidden layer output for every record - Shape of it \\n\", model1_hidden_output.shape)\n",
    "print(\"Few five records from hidden layer \\n\",model1_hidden_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We append the input words of the words2 column in the output of the h1 layer,this gives us the combined input representation\n",
    "\"\"\"\n",
    "word2_hidden_append = np.append(model1_hidden_output,word2_onehot, axis=1)\n",
    "print(\"word2_hidden_append Shape\", word2_hidden_append.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model2 = Sequential()\n",
    "ANN_model2.add(Dense(10, input_dim=word2_hidden_append.shape[1], activation='sigmoid'))\n",
    "ANN_model2.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "ANN_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model2.fit(word2_hidden_append, word3_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A predict function that takes input word1 and word2; and predict word3 \n",
    "#1. take the input word , and represent them using digits from the word_indices dictonary values\n",
    "#2. getting the intermediate hidden nodes for word1\n",
    "#3. appending hidden activations with word2 as final test set\n",
    "#4. prediction on this test set\n",
    "def two_step_pred(words_in):\n",
    "\n",
    "    index_input=word_indices[words_in[0]]\n",
    "    indices_in = keras.utils.to_categorical(index_input, num_classes=len(word_indices))\n",
    "    indices_in=indices_in.reshape(1,len(word_indices))\n",
    "    h1_test = model1_hidden.predict(indices_in) # getting our intermediate hidden activations from model1h\n",
    "    \n",
    "    \n",
    "    index_input2=word_indices[words_in[1]]\n",
    "    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(word_indices))\n",
    "    indices_in2= indices_in2.reshape(1,len(word_indices))\n",
    "    X2_test = np.append(h1_test, indices_in2, axis=1) #preparing final test data by appending hidden with word2\n",
    "    \n",
    "    yhat = ANN_model2.predict_classes(X2_test) #predicting final output from model2\n",
    "    \n",
    "    print(\"Input words --> \", words_in)\n",
    "    print(\"Predicted word --> \", indices_words[yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_step_pred(['love', 'it'])\n",
    "two_step_pred(['love', 'to'])\n",
    "two_step_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, use_bias=False, input_shape=(2,2)))\n",
    "model.add(Dense(3, use_bias=False, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(2,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(4,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word prediction using RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_word2 = input_3gram[['word1','word2']]\n",
    "for i in list(word1_word2.columns.values):\n",
    "    word1_word2[i] = word1_word2[i].map(word_indices)\n",
    "\n",
    "word1_word2=np.array(word1_word2)\n",
    "#The same data is reshaped with similar structure but appended with 1 value to make it 3d array\n",
    "word1_word2=np.reshape(word1_word2,(word1_word2.shape[0],2,1))\n",
    "word1_word2_onehot = keras.utils.to_categorical(np.array(word1_word2), num_classes=len(word_indices))\n",
    "print(\"word1_word2_onehot shape\", word1_word2_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time steps\" , word1_word2_onehot.shape[1])\n",
    "print(\"Input nodes\" , word1_word2_onehot.shape[2])\n",
    "print(\"output nodes\" , word3_onehot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "#model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, input_data_dim)))\n",
    "model_rnn.add(SimpleRNN(30, input_shape=(word1_word2_onehot.shape[1],word1_word2_onehot.shape[2]))) \n",
    "model_rnn.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_rnn.fit(word1_word2_onehot, word3_onehot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_word_pred(in_text):\n",
    "    print(\"Input is - \" , in_text)\n",
    "    encoded = [word_indices[i] for i in in_text]\n",
    "    encoded = np.array(encoded).reshape(1,2,1)\n",
    "    encoded =keras.utils.to_categorical(np.array(encoded), num_classes=len(word_indices))\n",
    "    ypred = model_rnn.predict_classes(encoded, verbose=0)[0]\n",
    "    print(\"Output is --> \" ,indices_words[ypred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_word_pred(['love', 'it'])\n",
    "rnn_word_pred(['love', 'to'])\n",
    "rnn_word_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longseq_3gram = open(Data_path+'\\\\Long_sequence_3gram.csv').read().lower()\n",
    "print(longseq_3gram[495:801])\n",
    "print(longseq_3gram[30615:31000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace comma with space\n",
    "longseq_3gram1= longseq_3gram.replace(',',' ').replace('\\r','')\n",
    "print(longseq_3gram1[495:750])\n",
    "print(longseq_3gram1[30615:30800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique characters in our dataset we then sort it\n",
    "chars = sorted(list(set(longseq_3gram1)))\n",
    "print(\"Unique Characters in the text \\n \",chars)\n",
    "#\\n is character string for new line, we dont need that in our dictionary of chars\n",
    "chars.remove('\\n')\n",
    "print(\"\\n Character after removing newline symbol \\'\\\\n\\'\",chars)\n",
    "print(\"\\n overall chars count\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "print(\"characters to indices dictionary\\n\", char_indices)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"indices to char dictionary\\n\", indices_char)\n",
    "print('unique chars: ', {len(chars)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char to index on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = longseq_3gram1.splitlines()\n",
    "##Adding a space at the end\n",
    "data = [i+' ' for i in data]\n",
    "\n",
    "##mapping our data into numbers\n",
    "sentences = [[char_indices[j] for j in i] for i in data ]\n",
    "print(data[0], sentences[0])\n",
    "print(data[10], sentences[1])\n",
    "print(data[20], sentences[2])\n",
    "print(data[100], sentences[3])\n",
    "print(data[400], sentences[400])\n",
    "print(data[4000], sentences[4000])\n",
    "print(data[9000], sentences[9000])\n",
    "##Number of sentences\n",
    "print(\"Number of sentences \", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the sentences to RNN friendly data which can be used to generate new sequences\n",
    "\n",
    "**Take one sentence, iterate through it till the length of sentence is reached:**\n",
    "\n",
    "* **Step 1** 0:0+14: X; and 14th position: y >> observation 1\n",
    "* **Step 2** 1:14: X; and 15th position: y >> observation 2\n",
    "* **Step 3** …we do this till the length of sentence is reached\n",
    "\n",
    "Take next sentence and repeat the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since all the sentences may not be of same length,it is neccessary to make them consistent when passing to keras\n",
    "#We select a sequence length\n",
    "Seq_ln = 14\n",
    "X = []\n",
    "y = []\n",
    "for i in sentences:\n",
    "    for j in range(len(i)-Seq_ln):\n",
    "        X.append(i[j:j+Seq_ln])\n",
    "        y.append(i[j+Seq_ln])\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data[0:2]=\", data[0:2])\n",
    "print(\"sentences[0:2]=\", sentences[0:2])\n",
    "\n",
    "for i in range (0,20):\n",
    "    print(\"X[\",i,\"]=\", X[i],\"y[\",i,\"]=\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first row is the X's first row up to 14 character\n",
    "#The second row is the X's first row starting from second character up to 14 character\n",
    "#The third row is the X's first row starting from third character up to 14 character and so on \n",
    "X=np.array(X)\n",
    "X1=np.reshape(X,(X.shape[0],X.shape[1],1))\n",
    "X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Variable\n",
    "y[:10]\n",
    "#Reshapig our label for model\n",
    "y1 = np.array(y)\n",
    "# one hot encode outputs\n",
    "y1 = keras.utils.to_categorical(np.array(y), num_classes=len(char_indices))\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "model_RNN2 = Sequential()\n",
    "##model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_RNN2.add(SimpleRNN(16, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_RNN2.add(Dense(len(char_indices)))\n",
    "model_RNN2.add(Activation('softmax'))\n",
    "model_RNN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=30, verbose=1, validation_data=(X_test, y_test))\n",
    "model_RNN2.save_weights(\"char_rnn_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsfile_model_RNN2= Data_path+\"\\\\Pre_trained_models\\\\char_rnn_model_weights_v1.hdf5\"\n",
    "model_RNN2.load_weights(weightsfile_model_RNN2)\n",
    "\n",
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input(in_text)\n",
    "        pred = model_RNN2.predict_classes(x, verbose=0)[0]\n",
    "\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = 'officials say '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted word ---> \", out_word)\n",
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "model_LSTM = Sequential()\n",
    "#model1.add(LSTM('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_LSTM.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_LSTM.add(Dense(len(char_indices)))\n",
    "model_LSTM.add(Activation('softmax'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train, epochs=30, verbose=1)\n",
    "model_LSTM.save_weights(\"char_LSTM_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsfile_model_LSTM= Data_path+\"\\\\Pre_trained_models\\\\char_LSTM_model_weights_v1.hdf5\"\n",
    "model_LSTM.load_weights( weightsfile_model_LSTM)\n",
    "\n",
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train,epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input1(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1= keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred1(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input1(in_text)\n",
    "        pred = model_LSTM.predict_classes(x, verbose=0)[0]\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = 'the emergence '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'officials say '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and sentenced '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'a combination '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and according '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few more predictions and Comparions with Standard RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study – Language Translation Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data= open(Data_path+\"\\\\fra-eng\\\\fra.txt\", mode='rt', encoding='utf-8').read()\n",
    "raw_data=raw_data.strip().split('\\n')\n",
    "raw_data=[i.split('\\t') for i in raw_data]\n",
    "lang1_lang2_data=array(raw_data)\n",
    "print(lang1_lang2_data)\n",
    "print(\"Overall pairs\", len(lang1_lang2_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "lang1_lang2_data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,0]]\n",
    "lang1_lang2_data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,1]]\n",
    "\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert text to lowercase\n",
    "for word in range(len(lang1_lang2_data)):\n",
    "    lang1_lang2_data[word,0] = lang1_lang2_data[word,0].lower()\n",
    "    lang1_lang2_data[word,1] = lang1_lang2_data[word,1].lower()\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 0])\n",
    "lang1_tokens=tokenizer\n",
    "lang1_vocab_size = len(lang1_tokens.word_index) + 1\n",
    "print(\"lang1_vocab_size\", lang1_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 1])\n",
    "lang2_tokens=tokenizer\n",
    "lang2_vocab_size = len(lang2_tokens.word_index) + 1\n",
    "print(\"lang2_vocab_size\", lang2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "train, test = train_test_split(lang1_lang2_data, test_size=0.1, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1_seq_length=15\n",
    "lang2_seq_length=15\n",
    "\n",
    "X_train_seq=lang1_tokens.texts_to_sequences(train[:, 0])\n",
    "X_train= pad_sequences(X_train_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_train_seq=lang2_tokens.texts_to_sequences(train[:, 1])\n",
    "Y_train= pad_sequences(Y_train_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "X_test_seq=lang1_tokens.texts_to_sequences(test[:, 0])\n",
    "X_test= pad_sequences(X_test_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_test_seq=lang2_tokens.texts_to_sequences(test[:, 1])\n",
    "Y_test= pad_sequences(Y_test_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"Y_train.shape\",Y_train.shape)\n",
    "print(\"X_test.shape\",X_test.shape)\n",
    "print(\"Y_test.shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text data\", [train[5, 0]])\n",
    "print('Numbers sequence', X_train_seq[5])\n",
    "print('Padded Sequence', X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(lang1_vocab_size, 256, input_length=lang1_seq_length, mask_zero=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(lang2_seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(lang2_vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),  epochs=1, verbose=1, batch_size=1024)\n",
    "model.save_weights('Eng_fra_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(Data_path+\"\\\\Pre_trained_models\\\\Eng_fra_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_line_prediction(text1):\n",
    "    \n",
    "    def to_lines(text):\n",
    "          sents = text.strip().split('\\n')\n",
    "          sents = [i.split('\\t') for i in sents]\n",
    "          return sents\n",
    "    small_input = to_lines(text1)\n",
    "    small_input = array(small_input)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n",
    "    # convert text to lowercase\n",
    "    for i in range(len(small_input)):\n",
    "        small_input[i,0] = small_input[i,0].lower()\n",
    "\n",
    "    #encode and pad sequences\n",
    "    small_input_seq=lang1_tokens.texts_to_sequences(small_input[0])\n",
    "    small_input= pad_sequences(small_input_seq,lang1_seq_length,padding='post')\n",
    "   \n",
    "\n",
    "    #Load the model\n",
    "    #Eng French Model\n",
    "    #model.load_weights('/content/drive/My Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/1.Archives/Eng_fra_model_v2.hdf5')\n",
    "\n",
    "    pred_seq = model.predict_classes(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])))\n",
    "    \n",
    "    def num_to_word(n, tokens):\n",
    "          for word, index in tokens.word_index.items():\n",
    "              if index == n:\n",
    "                  return word\n",
    "          return None\n",
    "\n",
    "    Lang2_text = []\n",
    "    for word_num in pred_seq:\n",
    "          sing_pred = []\n",
    "          for i in range(len(word_num)):\n",
    "                t = num_to_word(word_num[i], lang2_tokens)\n",
    "                if i > 0:\n",
    "                    if (t == num_to_word(word_num[i-1], lang2_tokens)) or (t == None):\n",
    "                        sing_pred.append('')\n",
    "                    else:\n",
    "                        sing_pred.append(t)\n",
    "                else:\n",
    "                      if(t == None):\n",
    "                              sing_pred.append('')\n",
    "                      else:\n",
    "                              sing_pred.append(t) \n",
    "          Lang2_text.append(' '.join(sing_pred))\n",
    "    return(Lang2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_sentences=[\"have a great Good day\",\n",
    "                 \"Do you speak English\",\n",
    "                 \"I do not know your language\",\n",
    "                 \"I need help\",\n",
    "                 \"Thank you very much\",\n",
    "                 \"Where can I get this\",\n",
    "                 \"How much does it cost\",\n",
    "                 \"Where is the bathroom\",\n",
    "                 \"Where is the ATM\",\n",
    "                 \"I am a visitor here\",\n",
    "                 \"Excuse me\",\n",
    "                 \"What do you do for living\",\n",
    "                 \"Here is my passport\"]\n",
    "\n",
    "for sent in Input_sentences:\n",
    "  print([sent] , \" -->\",one_line_prediction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
